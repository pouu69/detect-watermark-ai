# GPT 워터마크 탐지기 robots.txt

# 모든 검색 엔진에 대한 기본 규칙
User-agent: *
Allow: /

# 사이트맵 위치
Sitemap: https://gpt-scan.org/sitemap.xml

# 악성 봇 차단
User-agent: MJ12bot
Disallow: /

User-agent: AhrefsBot
Disallow: /

User-agent: BLEXBot
Disallow: /

User-agent: Baiduspider
Disallow: /

User-agent: Yandex
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: Rogerbot
Disallow: /

User-agent: Sogou web spider
Disallow: /

User-agent: Sogou inst spider
Disallow: /

User-agent: Exabot
Disallow: /

User-agent: ia_archiver
Disallow: /

User-agent: Alexibot
Disallow: /

User-agent: Bytespider
Disallow: /

User-agent: Applebot
Crawl-delay: 10

User-agent: YandexBot
Disallow: /

# 속도 제한 (크롤링 속도 제한)
User-agent: Googlebot
Crawl-delay: 5

User-agent: bingbot
Crawl-delay: 10

# 민감한 디렉토리 보호
Disallow: /admin/
Disallow: /api/
Disallow: /includes/
Disallow: /scripts/
Disallow: /temp/
Disallow: /cgi-bin/
Disallow: /backend/

# 참고: robots.txt는 DDoS 공격을 방지하는 보안 수단이 아닙니다.
# 실제 DDoS 방어를 위해서는 AWS WAF, CloudFlare 등의 서비스를 사용하세요.
